# –õ–∞–±–∏—Ä–∏–Ω—Ç —Å Reinforcement Learning

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ reinforcement learning –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ª–∞–±–∏—Ä–∏–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Gymnasium –∏ Stable Baselines3. –ü—Ä–æ–µ–∫—Ç –≤–∫–ª—é—á–∞–µ—Ç –∫–∞–∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –∏–≥—Ä—É –Ω–∞ pygame, —Ç–∞–∫ –∏ —Å—Ä–µ–¥—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤.

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

### –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã
- `Main_labirint.py` - –∏–≥—Ä–∞ –ª–∞–±–∏—Ä–∏–Ω—Ç –Ω–∞ pygame (—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä–µ–ª–∫–∞–º–∏)
- `maze_env.py` - —Å—Ä–µ–¥–∞ Gymnasium –¥–ª—è reinforcement learning
- `requirements.txt` - –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞

### –§–∞–π–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤
- `train_agent.py` - **–û–°–ù–û–í–ù–û–ô —Ñ–∞–π–ª –æ–±—É—á–µ–Ω–∏—è DQN –∞–≥–µ–Ω—Ç–∞** (—Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏)
- `simple_train_agent.py` - —É–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ PPO –∞–≥–µ–Ω—Ç–∞
- `test_agent.py` - —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
- `test_env.py` - —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ä–µ–¥—ã

### –§–∞–π–ª—ã –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
- `agent_training_visualize.py` - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–Ω–æ–≥–æ DQN –∞–≥–µ–Ω—Ç–∞
- `simple_agent_vizualaiser.py` - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–Ω–æ–≥–æ PPO –∞–≥–µ–Ω—Ç–∞

### –ú–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã–µ
- `models/` - –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
- `ppo_maze_agent.zip` - –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è PPO –º–æ–¥–µ–ª—å
- `ppo-maze-10_000.zip` - –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è PPO –º–æ–¥–µ–ª—å (10k —à–∞–≥–æ–≤)

##  –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
```bash
pip install -r requirements.txt
```

### 2. –ò–≥—Ä–∞ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ª–∞–±–∏—Ä–∏–Ω—Ç
```bash
python Main_labirint.py
```
**–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ**: —Å—Ç—Ä–µ–ª–∫–∏ –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã

### 3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ä–µ–¥—ã
```bash
python test_env.py
```

### 4. –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ (–≤—ã–±–µ—Ä–∏—Ç–µ –æ–¥–∏–Ω –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤)

#### –í–∞—Ä–∏–∞–Ω—Ç A: –ü—Ä–æ—Å—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ PPO (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
```bash
python simple_train_agent.py
```

#### –í–∞—Ä–∏–∞–Ω—Ç B: –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ DQN (—Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏)
```bash
python train_agent.py
```

### 5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

#### –î–ª—è PPO –∞–≥–µ–Ω—Ç–∞:
```bash
python simple_agent_vizualaiser.py
```

#### –î–ª—è DQN –∞–≥–µ–Ω—Ç–∞:
```bash
python agent_training_visualize.py
```

### 6. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
```bash
python test_agent.py
```

##  –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

### –ü—Ä–æ–±–ª–µ–º—ã —Å `train_agent.py`
–û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª –æ–±—É—á–µ–Ω–∏—è `train_agent.py` –∏–º–µ–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã:

1. **–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**: DQN –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –Ω–µ —Å—Ö–æ–¥–∏—Ç—å—Å—è –∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º—É —Ä–µ—à–µ–Ω–∏—é
2. **–ü—Ä–æ–±–ª–µ–º—ã —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏**: —Ç–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–π —Å—Ä–µ–¥—ã
3. **–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏**: –Ω–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ
4. **–ü—Ä–æ–±–ª–µ–º—ã —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º**: –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `simple_train_agent.py` –∫–∞–∫ –æ—Å–Ω–æ–≤—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
- –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (learning_rate, buffer_size, etc.)
- –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è (—É–≤–µ–ª–∏—á–∏—Ç—å `total_timesteps`)
- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å callback –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

##  –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç Reinforcement Learning

### –°—Ä–µ–¥–∞ (Environment)
- **–°–æ—Å—Ç–æ—è–Ω–∏–µ**: –ø–æ–∑–∏—Ü–∏—è –∞–≥–µ–Ω—Ç–∞ –≤ –ª–∞–±–∏—Ä–∏–Ω—Ç–µ (x, y –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)
- **–î–µ–π—Å—Ç–≤–∏—è**: 4 –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è (0-–≤–≤–µ—Ä—Ö, 1-–≤–ø—Ä–∞–≤–æ, 2-–≤–Ω–∏–∑, 3-–≤–ª–µ–≤–æ)
- **–ù–∞–≥—Ä–∞–¥—ã**: 
  - `+100` –∑–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Ñ–∏–Ω–∏—à–∞
  - `-1` –∑–∞ —Å—Ç–æ–ª–∫–Ω–æ–≤–µ–Ω–∏–µ —Å–æ —Å—Ç–µ–Ω–æ–π
  - `-0.1` –∑–∞ –∫–∞–∂–¥—ã–π —Ö–æ–¥ (—à—Ç—Ä–∞—Ñ –∑–∞ –≤—Ä–µ–º—è)

### –ê–ª–≥–æ—Ä–∏—Ç–º—ã
- **DQN** (Deep Q-Network) - –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
- **PPO** (Proximal Policy Optimization) - –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º

### –õ–∞–±–∏—Ä–∏–Ω—Ç
```
S - —Å—Ç–∞—Ä—Ç (0,0), F - —Ñ–∏–Ω–∏—à (9,9)
S . # . . . . . . .
. . # . # # # # . .
. . # . # . . # . .
. . # . # . # # . .
. . . . . . # . . .
. # # # . # # # # .
. # . # . . . . # .
. # . # # # # . # .
. . . . . . . . # .
# # # # # # # # # F
```

##  –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

### PPO –∞–≥–µ–Ω—Ç (10,000 —à–∞–≥–æ–≤)
- –£—Å–ø–µ—à–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç –ø—É—Ç—å –∫ —Ñ–∏–Ω–∏—à—É
- –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: ~20-30 —à–∞–≥–æ–≤
- –°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

### DQN –∞–≥–µ–Ω—Ç (10,000 —à–∞–≥–æ–≤)
- –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
- –ú–µ–Ω–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- –ù—É–∂–¥–∞–µ—Ç—Å—è –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

## üîß –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏

### –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
- `gymnasium==0.29.1` - —Å—Ä–µ–¥–∞ –¥–ª—è RL
- `stable-baselines3==2.1.0` - –∞–ª–≥–æ—Ä–∏—Ç–º—ã RL
- `torch==2.1.0` - PyTorch –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π
- `pygame==2.5.2` - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
- `numpy==1.24.3` - —á–∏—Å–ª–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
- `matplotlib==3.7.2` - –≥—Ä–∞—Ñ–∏–∫–∏

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ä–µ–¥—ã
- –†–∞–∑–º–µ—Ä –ª–∞–±–∏—Ä–∏–Ω—Ç–∞: 10x10
- –†–∞–∑–º–µ—Ä —è—á–µ–π–∫–∏: 50x50 –ø–∏–∫—Å–µ–ª–µ–π
- –û–∫–Ω–æ: 500x500 –ø–∏–∫—Å–µ–ª–µ–π

## üéØ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

1. **–ò—Å–ø—Ä–∞–≤–∏—Ç—å `train_agent.py`**:
   - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
   - –î–æ–±–∞–≤–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è
   - –£–ª—É—á—à–∏—Ç—å —Å–∏—Å—Ç–µ–º—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π

2. **–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏**:
   - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω—ã—Ö –ª–∞–±–∏—Ä–∏–Ω—Ç–æ–≤
   - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
   - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è

3. **–£–ª—É—á—à–∏—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é**:
   - –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
   - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
   - –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è


## –õ–∏—Ü–µ–Ω–∑–∏—è

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–ª–µ–π. 

---

# Maze with Reinforcement Learning (English)

This project demonstrates the use of reinforcement learning to solve a maze using Gymnasium and Stable Baselines3. The project includes both the original maze game on pygame and an environment for training AI agents.

## Project Structure

### Main files
- `Main_labirint.py` - the original maze game on pygame (arrow key controls)
- `maze_env.py` - Gymnasium environment for reinforcement learning
- `requirements.txt` - project dependencies

### Agent training files
- `train_agent.py` - **MAIN DQN agent training script** (**needs improvement, see below**)
- `simple_train_agent.py` - simplified PPO agent training
- `test_agent.py` - test a trained agent
- `test_env.py` - test the environment

### Visualization files
- `agent_training_visualize.py` - visualize a trained DQN agent
- `simple_agent_vizualaiser.py` - visualize a trained PPO agent

### Models and data
- `models/` - directory for saved models
- `ppo_maze_agent.zip` - pretrained PPO model
- `ppo-maze-10_000.zip` - pretrained PPO model (10k steps)

## Quick Start

### 1. Install dependencies
```bash
pip install -r requirements.txt
```

### 2. Play the original maze game
```bash
python Main_labirint.py
```
**Controls**: arrow keys

### 3. Test the environment
```bash
python test_env.py
```

### 4. Train an agent (choose one)

#### Option A: Simple PPO training (recommended)
```bash
python simple_train_agent.py
```

#### Option B: Full DQN training (**needs improvement**)
```bash
python train_agent.py
```

### 5. Visualize results

#### For PPO agent:
```bash
python simple_agent_vizualaiser.py
```

#### For DQN agent:
```bash
python agent_training_visualize.py
```

### 6. Test a trained agent
```bash
python test_agent.py
```

## Known Issues

### Problems with `train_agent.py`
The main training script `train_agent.py` has the following issues:

1. **Unstable training**: DQN agent may not converge to an optimal solution
2. **Hyperparameter issues**: current settings may be suboptimal for this environment
3. **No validation**: no quality check during training
4. **Saving issues**: model may not save correctly

### Recommendations
- Use `simple_train_agent.py` as a base for training
- Experiment with hyperparameters (learning_rate, buffer_size, etc.)
- Increase the number of training steps (`total_timesteps`)
- Add callbacks for monitoring progress

## How Reinforcement Learning Works

### Environment
- **State**: agent's position in the maze (x, y coordinates)
- **Actions**: 4 movement directions (0-up, 1-right, 2-down, 3-left)
- **Rewards**:
  - `+100` for reaching the finish
  - `-1` for hitting a wall
  - `-0.1` for each step (time penalty)

### Algorithms
- **DQN** (Deep Q-Network) - for discrete actions
- **PPO** (Proximal Policy Optimization) - more stable for this task

### Maze layout
```
S - start (0,0), F - finish (9,9)
S . # . . . . . . .
. . # . # # # # . .
. . # . # . . # . .
. . # . # . # # . .
. . . . . . # . . .
. # # # . # # # # .
. # . # . . . . # .
. # . # # # # . # .
. . . . . . . . # .
# # # # # # # # # F
```

## Results

### PPO agent (10,000 steps)
- Successfully finds a path to the finish
- Average time: ~20-30 steps
- Stable training

### DQN agent (10,000 steps)
- Needs more time to converge
- Less stable results
- Requires hyperparameter tuning

## Technical Details

### Dependencies
- `gymnasium==0.29.1` - RL environment
- `stable-baselines3==2.1.0` - RL algorithms
- `torch==2.1.0` - PyTorch for neural networks
- `pygame==2.5.2` - visualization
- `numpy==1.24.3` - numerical computations
- `matplotlib==3.7.2` - plots

### Environment structure
- Maze size: 10x10
- Cell size: 50x50 pixels
- Window: 500x500 pixels

## Next Steps

1. **Fix `train_agent.py`**:
   - Optimize hyperparameters
   - Add training validation
   - Improve model saving

2. **Add new features**:
   - Random maze generation
   - Algorithm comparison
   - Training process visualization

3. **Improve visualization**:
   - Training graphs
   - Success statistics
   - Interactive demo

## License

This project is for educational purposes only. 